{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27eda919-ad0d-4b7c-a4b4-9adb4b076beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score,accuracy_score,recall_score,f1_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d491c6f5-5fe8-42a3-bd9b-1769e10cd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the loan approval dataset\n",
    "loan_df = pd.read_csv(\"loan_approval_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbbce610-fde5-4f4a-8e5c-4d34676d8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unique identifier as it does not contribute to prediction\n",
    "loan_df=loan_df.drop(columns=['Applicant_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eedc3de-d5b3-4a94-88f1-371f841beac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns (float type) to handle missing values\n",
    "# separately using appropriate numerical imputation strategies\n",
    "num_mis_val = loan_df.select_dtypes(include=['float64']).columns\n",
    "\n",
    "# Identify categorical columns (object type) for separate\n",
    "# categorical missing value treatment (e.g., mode imputation)\n",
    "cat_mis_val = loan_df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9190801a-d507-41d9-8474-65e5a7ac0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i learnt this new method of filling the missing value\n",
    "\n",
    "# Handle missing values using SimpleImputer\n",
    "# Numerical features are imputed with the mean to preserve overall distribution\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute missing values in numerical columns\n",
    "num_imp = SimpleImputer(strategy='mean')\n",
    "loan_df[num_mis_val] = num_imp.fit_transform(loan_df[num_mis_val])\n",
    "\n",
    "# Impute missing values in categorical columns\n",
    "# Most frequent value (mode) is used to retain category consistency\n",
    "cat_imp = SimpleImputer(strategy='most_frequent')\n",
    "loan_df[cat_mis_val] = cat_imp.fit_transform(loan_df[cat_mis_val])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558fe01-4aa9-4130-9f48-1a3a76c0fcd5",
   "metadata": {},
   "source": [
    "# Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c78a023c-9f64-4fce-8907-b7607322f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables into numerical form using LabelEncoder\n",
    "# This is required as machine learning models cannot work with string values\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # new thing\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode Education_Level (ordinal/binary categorical feature)\n",
    "loan_df['Education_Level'] = le.fit_transform(loan_df['Education_Level'])\n",
    "\n",
    "# Encode target variable (Loan_Approved) into binary numerical labels\n",
    "# This enables supervised learning model training\n",
    "loan_df['Loan_Approved'] = le.fit_transform(loan_df['Loan_Approved'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfeaff11-8d04-4ac9-af34-76160db0f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply One-Hot Encoding to nominal categorical features\n",
    "# OneHotEncoder is preferred here to avoid introducing\n",
    "# artificial ordinal relationships between categories\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# List of categorical columns to be one-hot encoded\n",
    "col = [\n",
    "    'Employment_Status',\n",
    "    'Marital_Status',\n",
    "    'Loan_Purpose',\n",
    "    'Property_Area',\n",
    "    'Gender',\n",
    "    'Employer_Category'\n",
    "]\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "# - drop='first' helps reduce multicollinearity (dummy variable trap)\n",
    "# - sparse_output=False returns a dense NumPy array\n",
    "# - handle_unknown='ignore' ensures robustness during inference\n",
    "ohe = OneHotEncoder(\n",
    "    drop='first',\n",
    "    sparse_output=False,\n",
    "    handle_unknown='ignore'\n",
    ")\n",
    "\n",
    "# Fit encoder on categorical features and transform them\n",
    "encoded = ohe.fit_transform(loan_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac47d5e1-05be-407a-b8e2-165353899722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert encoded NumPy array into a DataFrame with meaningful column names\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded,\n",
    "    columns=ohe.get_feature_names_out(col),\n",
    "    index=loan_df.index\n",
    ")\n",
    "\n",
    "# Drop original categorical columns and concatenate encoded features\n",
    "# to form the final preprocessed dataset\n",
    "loan_df = pd.concat(\n",
    "    [loan_df.drop(columns=col), encoded_df],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8303b33-4c9b-44d2-877b-4eb4d5b55edd",
   "metadata": {},
   "source": [
    "# Train-Test-split + Feature Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1d3b6f2-c419-416b-85d8-5eeb1ad81e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input features (X) and target variable (y)\n",
    "# Loan_Approved is the label we want the model to predict\n",
    "X = loan_df.drop(columns=['Loan_Approved'],axis=1)\n",
    "y = loan_df['Loan_Approved']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# Test size of 20% ensures fair evaluation on unseen data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "843a2179-c910-4bca-87a3-9bc285277e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StandardScaler to normalize feature values\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler ONLY on training data to learn scaling parameters\n",
    "# This prevents data leakage from the test set\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same scaling transformation to test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d074e-fd2c-45d4-8446-f0afaff0b2f8",
   "metadata": {},
   "source": [
    "# Train & Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "955f5838-822a-4590-9678-495165f0fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation For Naive Bayes\n",
      "Precision: 0.8035714285714286\n",
      "Accuracy: 0.865\n",
      "Recall: 0.7377049180327869\n",
      "F1: 0.7692307692307693\n",
      "cm: [[128  11]\n",
      " [ 16  45]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gaussian Naive Bayes classifier\n",
    "# Suitable for continuous features assuming normal distribution\n",
    "gb_model = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes model on scaled training data\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Generate predictions on scaled test data\n",
    "y_pred = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics to assess Naive Bayes model performance\n",
    "print(\"Evaluation For Naive Bayes\")\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('F1:', f1_score(y_test, y_pred))\n",
    "print('cm:', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255a17f-5012-489d-98ff-e18d4608ad23",
   "metadata": {},
   "source": [
    "### Final Model Selection\n",
    "\n",
    "Multiple machine learning models were trained and evaluated, including\n",
    "Logistic Regression, KNN, and Naive Bayes.\n",
    "\n",
    "Based on comparative performance across accuracy, precision, recall,\n",
    "and F1-score, Naive Bayes demonstrated the most consistent results\n",
    "and was selected as the final model for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383aed19-184a-4646-8dd7-31e2d619f273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
